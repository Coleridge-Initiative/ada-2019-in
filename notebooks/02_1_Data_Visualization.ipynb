{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Ghani, Rayid, Frauke Kreuter, Julia Lane, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Brian Kim, Avishek Kumar, Jonathan Morgan, and Ridhima Sodhi. \n",
    "\n",
    "_source to be updated when notebook added to GitHub_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization in Python\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Data-Visualization-in-Python\" data-toc-modified-id=\"Data-Visualization-in-Python-1\">Data Visualization in Python</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\">Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning-Objectives\" data-toc-modified-id=\"Learning-Objectives-1.1.1\">Learning Objectives</a></span></li></ul></li><li><span><a href=\"#Python-Setup\" data-toc-modified-id=\"Python-Setup-1.2\">Python Setup</a></span></li><li><span><a href=\"#Load-the-Data\" data-toc-modified-id=\"Load-the-Data-1.3\">Load the Data</a></span></li><li><span><a href=\"#Visual-data-exploration-with-matplotlib\" data-toc-modified-id=\"Visual-data-exploration-with-matplotlib-1.4\">Visual data exploration with <code>matplotlib</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#A-Note-on-Data-Sourcing\" data-toc-modified-id=\"A-Note-on-Data-Sourcing-1.4.1\">A Note on Data Sourcing</a></span></li><li><span><a href=\"#Layering-in-matplotlib\" data-toc-modified-id=\"Layering-in-matplotlib-1.4.2\">Layering in <code>matplotlib</code></a></span></li></ul></li><li><span><a href=\"#Introducing-seaborn\" data-toc-modified-id=\"Introducing-seaborn-1.5\">Introducing <code>seaborn</code></a></span><ul class=\"toc-item\"><li><span><a href=\"#Combining-seaborn-and-matplotlib\" data-toc-modified-id=\"Combining-seaborn-and-matplotlib-1.5.1\">Combining <code>seaborn</code> and <code>matplotlib</code></a></span></li></ul></li><li><span><a href=\"#Exploring-cohort-employment\" data-toc-modified-id=\"Exploring-cohort-employment-1.6\">Exploring cohort employment</a></span><ul class=\"toc-item\"><li><span><a href=\"#A-heatmap-using-Seaborn\" data-toc-modified-id=\"A-heatmap-using-Seaborn-1.6.1\">A heatmap using Seaborn</a></span></li><li><span><a href=\"#Full-quarter-employment\" data-toc-modified-id=\"Full-quarter-employment-1.6.2\">Full quarter employment</a></span></li></ul></li><li><span><a href=\"#Exporting-Completed-Graphs\" data-toc-modified-id=\"Exporting-Completed-Graphs-1.7\">Exporting Completed Graphs</a></span></li><li><span><a href=\"#Choosing-a-Data-Visualization-Package\" data-toc-modified-id=\"Choosing-a-Data-Visualization-Package-1.8\">Choosing a Data Visualization Package</a></span><ul class=\"toc-item\"><li><span><a href=\"#An-Important-Note-on-Graph-Titles\" data-toc-modified-id=\"An-Important-Note-on-Graph-Titles-1.8.1\">An Important Note on Graph Titles</a></span></li></ul></li><li><span><a href=\"#Additional-Resources\" data-toc-modified-id=\"Additional-Resources-1.9\">Additional Resources</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this module, you will learn to quickly and flexibly make a wide series of visualizations for exploratory data analysis and communicating to your audience. This module contains a practical introduction to data visualization in Python and covers important rules that any data visualizer should follow.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "* Become familiar with a core base of data visualization tools in Python - specifically matplotlib and seaborn\n",
    "\n",
    "* Begin exploring what visualizations are going to best reveal various types of patterns in your data\n",
    "\n",
    "* Learn more about our primary datasets data with exploratory analyses and visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation in Python\n",
    "import pandas as pd\n",
    "\n",
    "# visualization packages\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# database connection\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# see how long queries/etc take\n",
    "import time\n",
    "\n",
    "# so images get plotted in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "- Back to [Table of Contents](#Table-of-Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up sqlalchemy engine\n",
    "host = 'stuffed.adrf.info'\n",
    "DB = 'appliedda'\n",
    "\n",
    "connection_string = \"postgresql://{}/{}\".format(host, DB)\n",
    "conn = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue exploring a similar selection of data as we ended with in the [Dataset Exploration](02_2_Dataset_Exploration.ipynb) notebook.\n",
    "\n",
    "**SQL code to generate the tables we'll explore below**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = 'data_ohio_olda_2018'\n",
    "tbl = 'oh_hei'\n",
    "\n",
    "query = '''\n",
    "SELECT column_name\n",
    "FROM information_schema.columns \n",
    "WHERE table_schema = '{}' AND table_name = '{}'\n",
    "'''.format(schema, tbl)\n",
    "\n",
    "# read results\n",
    "hei_columns = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dataset contains {} columns'.format(hei_columns.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define which columns from the table we want\n",
    "select_columns = [c for c in hei_columns['column_name'].values if c.startswith('deg')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### explore 2010 HEI data\n",
    "\n",
    "# code run to generate 2010 graduate table cohort\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# sql = \"\"\"\n",
    "# CREATE TABLE data_ohio_olda_2018.oh_hei_grad2010 AS\n",
    "# SELECT *\n",
    "# FROM (SELECT key_id, file_year AS year,\n",
    "#         unnest(array[{col_str}]) AS col_name,\n",
    "#         unnest(array[{col_val}]) AS col_value\n",
    "#     FROM data_ohio_olda_2018.oh_hei\n",
    "#     WHERE file_year = 2010\n",
    "#     ) sub_query\n",
    "# WHERE col_value IS NOT NULL AND col_value <> ''\n",
    "# \"\"\".format(\n",
    "# col_str=','.join([\"'\"+c+\"'\" for c in select_columns]), #deg_cols]),\n",
    "# col_val=','.join([c+'::text' for c in select_columns]) #deg_cols])\n",
    "# )\n",
    "## run sql\n",
    "# conn.execute(sql)\n",
    "\n",
    "\n",
    "print('query completed in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create index on \"key_id\" for joins to other tables\n",
    "# conn.execute('CREATE INDEX ON data_ohio_olda_2018.oh_hei_grad2010 (key_id)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql('SELECT * FROM data_ohio_olda_2018.oh_hei_grad2010', conn)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inst_columns = [c for c in df['col_name'].unique() if c.endswith('_inst')]\n",
    "subject_columns = [c for c in df['col_name'].unique() if 'subject' in c]\n",
    "\n",
    "print(inst_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('In this sample, {:,.0f} individuals graduated from {} different institutions and studied {} subjects'\\\n",
    ".format(df['key_id'].nunique(),\n",
    "        df[df['col_name'].isin(inst_columns)]['col_value'].nunique(),\n",
    "        df[df['col_name'].isin(subject_columns)]['col_value'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get jobs worked by our cohort\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# query = \"\"\"\n",
    "# CREATE TABLE data_ohio_olda_2018.oh_hei_grad2010_jobs AS\n",
    "# SELECT * \n",
    "# FROM data_ohio_olda_2018.oh_ui_wage_by_employer\n",
    "# WHERE key_id IN (SELECT key_id FROM data_ohio_olda_2018.oh_hei_grad2010)\n",
    "# --AND year = 2015\n",
    "# \"\"\"\n",
    "# conn.execute(query)\n",
    "\n",
    "# report how long reading this data frame took\n",
    "print('query ran in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the jobs data for 2015, and parse the dates so we can use datetime functions\n",
    "df_jobs = pd.read_sql(\"SELECT * FROM data_ohio_olda_2018.oh_hei_grad2010_jobs WHERE year = 2015\", conn)\n",
    "\n",
    "df_jobs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_grad = df['key_id'].nunique()\n",
    "num_empl = df_jobs['key_id'].nunique()\n",
    "print('of {:,.0f} graduates, {:,.0f} ({:.1f}%) had at least one job in 2015'.format(num_grad,num_empl, \n",
    "                                                                                    num_empl/num_grad*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual data exploration with `matplotlib`\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Under the hood, `Pandas` uses `matplotlib` to produce visualizations. `matplotlib` is the most commonly used base visualization package and provides low level access to different chart characteristics (eg tick mark labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple distribution\n",
    "df_jobs.hist(column='wages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple distribution with specified number of bins\n",
    "df_jobs.hist(column='wages', bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the simple histogram produced above shows a l/ot of small earnings values\n",
    "# what is the distribution of the higher values\n",
    "df_jobs['wages'].describe(percentiles = [.01, .1, .25, .5, .75, .9, .95, .99, .999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can see a long tail in the earnings per job\n",
    "## let's subset to below the 99% percentile and make a historgram\n",
    "subset_values = df_jobs['wages']<pd.np.percentile(df_jobs['wages'], 99)\n",
    "\n",
    "df_jobs[subset_values].hist(column='wages', bins=50, grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note in the above cell we split subsetting the data into two steps:\n",
    "1. We created `subset_values` which is simply a list of True or False\n",
    "2. Then we selected all rows in the  `df_jobs` dataframe where `subset_values` was True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can change options within the hist function (e.g. number of bins, color, transparency):\n",
    "df_jobs[subset_values].hist(column='wages', bins=20, facecolor=\"purple\", alpha=0.5, figsize=(10,6), grid=False)\n",
    "\n",
    "## And we can change the plot options with `plt` (which is our alias for matplotlib.pyplot)\n",
    "plt.xlabel('Job earnings ($)')\n",
    "plt.ylabel('Number of jobs')\n",
    "plt.title('Distribution of jobs by earnings for the cohort')\n",
    "\n",
    "## And add Data sourcing:\n",
    "### xy are measured in percent of axes length, from bottom left of graph:\n",
    "plt.annotate('Source: Ohio Longitudinal Data Archive', \n",
    "             xy=(0.5,-0.15), xycoords=\"axes fraction\")\n",
    "\n",
    "## We use plt.show() to display the graph once we are done setting options:\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Note on Data Sourcing\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Data sourcing is a critical aspect of any data visualization. Although here we are simply referencing the agencies that created the data, it is ideal to provide as direct of a path as possible for the viewer to find the data the graph is based on. When this is not possible (e.g. the data is sequestered), directing the viewer to documentation or methodology for the data is a good alternative. Regardless, providing clear sourcing for the underlying data is an **absolutely requirement** of any respectable visualization, and further builds trusts and enables reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layering in `matplotlib`\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "As briefly demonstrated by changing the labels and adding the source, above, we can make consecutive changes to the same plot; that means we can also layer multiple plots on the same `figure`. By default, the first graph you create will be on the bottom with following graphs on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# demonstrate simple layering\n",
    "\n",
    "plt.hist(df_jobs[subset_values & (df_jobs['quarter']==2)]['wages'], facecolor=\"blue\", alpha=0.6)\n",
    "plt.hist(df_jobs[subset_values & (df_jobs['quarter']==3)]['wages'], facecolor=\"orange\", alpha=0.6)\n",
    "\n",
    "plt.annotate('Source: Ohio Longitudinal Data Archive', \n",
    "             xy=(0.5,-0.15), xycoords=\"axes fraction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing `seaborn`\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "`Seaborn` is a popular visualization package built on top of `matplotlib` which makes some more cumbersome graphs easier to make, however it does not give direct access to the lower level objects in a `figure` (more on that later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Barplot function in seaborn\n",
    "sns.barplot(x='quarter', y='wages', data=df_jobs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What values does the above plot actually show us? Let's use the `help()` function to check the details of the `seaborn.barplot()` function we called above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(sns.barplot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the documentation, we can see that there is an `estimator` function that by default is `mean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Barplot using sum of earnings rather than the default mean\n",
    "sns.barplot(x='quarter', y='wages', data=df_jobs, estimator=sum)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Barplot using median of earnings\n",
    "sns.barplot(x='quarter', y='wages', data=df_jobs, estimator=pd.np.median)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Seaborn has a great series of charts for showing different cuts of data\n",
    "sns.factorplot(x='quarter', y='wages', hue='employer_num', data=df_jobs, kind='bar')\n",
    "plt.show()\n",
    "\n",
    "## Other options for the 'kind' argument can be found in the documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining `seaborn` and `matplotlib` \n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "There are many excellent data visualiation modules available in Python, but for the tutorial we will stick to the tried and true combination of `matplotlib` and `seaborn`.\n",
    "\n",
    "Below, we use `seaborn` for setting an overall aesthetic style and then faceting (created small multiples). We then use `matplotlib` to set very specific adjustments - things like adding the title, adjusting the locations of the plots, and sizing th graph space. This is a pretty protoyptical use of the power of these two libraries together. \n",
    "\n",
    "More on [`seaborn`'s set_style function](https://seaborn.pydata.org/generated/seaborn.set_style.html).\n",
    "More on [`matplotlib`'s figure (fig) API](https://matplotlib.org/api/figure_api.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Seaborn offers a powerful tool called FacetGrid for making small multiples of matplotlib graphs:\n",
    "\n",
    "### Create an empty set of grids:\n",
    "facet_histograms = sns.FacetGrid(df_jobs[subset_values], row='employer_num', col='quarter')\n",
    "\n",
    "## \"map' a histogram to each grid:\n",
    "facet_histograms = facet_histograms.map(plt.hist, 'wages')\n",
    "\n",
    "## Data Sourcing:\n",
    "plt.annotate('Source: OLDA', \n",
    "             xy=(0.5,-0.35), xycoords=\"axes fraction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn's set_style function allows us to set many aesthetic parameters.\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "### Create an empty set of grids:\n",
    "facet_histograms = sns.FacetGrid(df_jobs[subset_values], row='employer_num', col='quarter')\n",
    "## \"map' a histogram to each grid:\n",
    "facet_histograms = facet_histograms.map(plt.hist, 'wages')\n",
    "\n",
    "## We can still change options with matplotlib, using facet_histograms.fig\n",
    "facet_histograms.fig.subplots_adjust(top=0.9)\n",
    "facet_histograms.fig.suptitle(\"Earnings for 99% of the jobs held by the cohort\", fontsize=14)\n",
    "facet_histograms.fig.set_size_inches(12,8)\n",
    "\n",
    "## Data Sourcing:\n",
    "facet_histograms.fig.text(x=0.5, y=-0.05, s='Source: OLDA',\n",
    "                         fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring cohort employment\n",
    "\n",
    "Question: what are employment patterns of our cohort?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder of what columns we have in our two DataFrames\n",
    "print(df.columns.tolist())\n",
    "print('') # just to add a space\n",
    "print(df_jobs.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also check the total rows in the two datasets, and the number of unique individuals in our jobs data\n",
    "print(df.shape[0], df['key_id'].nunique())\n",
    "print(df_jobs.shape[0], df_jobs['key_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many in our cohort had any job during each quarter\n",
    "df_jobs.groupby(['year', 'quarter'])['key_id'].nunique().plot(kind='bar', grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# did individuals have more than one job in a given quarter?\n",
    "df_jobs.groupby(['year', 'quarter', 'key_id'])['employer'].count().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter one of the key_id values here to see what the underlying data looks like\n",
    "key_id_to_view = 'replace-this-text-with-key-id'\n",
    "\n",
    "df_jobs[df_jobs['key_id']==key_id_to_view]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of jobs each individual had in each quarter\n",
    "# where a \"job\" is simply that they had a record in the data\n",
    "df_tmp = df_jobs.groupby(['year', 'quarter', 'key_id'])['employer'].count().unstack(['year', 'quarter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten all columns to a single name with an '_' separator:\n",
    "df_tmp.columns = ['_'.join([str(c) for c in col]) for col in df_tmp.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN with 0\n",
    "df_tmp.fillna(0, inplace=True)\n",
    "\n",
    "# and set values >0 to 1\n",
    "df_tmp[df_tmp>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make ID value a column instead of an index - then we can count it when we group by the 'year_q' columns\n",
    "df_tmp.reset_index(inplace=True)\n",
    "df_tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list of just the columns that start with '2015'\n",
    "cols = [c for c in df_tmp.columns.values if c.startswith('2015')]\n",
    "\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aside on the above \"list comprehension\", here are the same steps one by one:\n",
    "\n",
    "# 1- get an array of our columns\n",
    "column_list = df_tmp.columns.values\n",
    "\n",
    "# 2 - loop through each value in the array\n",
    "for c in column_list:\n",
    "    # 3 - check if the string starts with either '2005' or '2006'\n",
    "    if c.startswith('2015'):\n",
    "        # 4 - add the column to our new list (here we just print to demonstrate)\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by all columns to count number of people with the same pattern\n",
    "df_tmp = df_tmp.groupby(cols)['key_id'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename key_id to make it less confusing later\n",
    "df_tmp.rename(columns={'key_id': 'count_keyid'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} different patterns of employment in 2015'.format(df_tmp.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total possible patterns of employment\n",
    "poss_patterns = 2**len(cols)\n",
    "\n",
    "pct_of_patterns = 100 * df_tmp.shape[0] / poss_patterns\n",
    "\n",
    "print('With this definition of employment, our cohort shows {:.1f}% of the possible patterns'.format(pct_of_patterns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at just the top 10:\n",
    "df_tmp.sort_values('count_keyid', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and how many people follow other patterns\n",
    "df_tmp.sort_values('count_keyid', ascending=False).tail(df_tmp.shape[0]-10)['count_keyid'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the top 10 for a visualization\n",
    "df_tmp_top = df_tmp.sort_values('count_keyid', ascending=False).head(10).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop old index\n",
    "df_tmp_top.drop(columns='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('percent of employed in top 10 patterns is {:.1f}%'\\\n",
    ".format(100*df_tmp_top['count_keyid'].sum()/df_tmp['count_keyid'].sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentage of cohort in each group:\n",
    "df_tmp_top['pct_cohort'] = df_tmp_top['count_keyid'].astype(float) / df['key_id'].nunique()\n",
    "df_tmp_top.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A heatmap using Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize with a simple heatmap\n",
    "sns.heatmap(df_tmp_top[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default visualization leaves a lot to be desired. Now let's customize the same heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the matplotlib object so we can tweak graph properties later\n",
    "fig, ax = plt.subplots(figsize = (14,8))\n",
    "\n",
    "# create the list of labels we want on our y-axis\n",
    "ylabs = ['{:.2f}%'.format(x*100) for x in df_tmp_top['pct_cohort']]\n",
    "\n",
    "# make the heatmap\n",
    "sns.heatmap(df_tmp_top[cols], linewidths=0.01, linecolor='grey', yticklabels=ylabs, cbar=False, cmap=\"Blues\")\n",
    "\n",
    "# make y-labels horizontal and change tickmark font size\n",
    "plt.yticks(rotation=360, fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "\n",
    "# add axis labels\n",
    "ax.set_ylabel('Percent of cohort', fontsize=16)\n",
    "ax.set_xlabel('Quarter', fontsize=16)\n",
    "\n",
    "## Data Sourcing:\n",
    "ax.annotate('Source: Ohio Longitudinal Data Archive', \n",
    "            xy=(0.5,-0.15), xycoords=\"axes fraction\", fontsize=12)\n",
    "\n",
    "## add a title\n",
    "fig.suptitle('Top 10 most common employment patterns of cohort', fontsize=18)\n",
    "ax.set_title('Blue is \"employed\" and white is \"not employed\"', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full quarter employment\n",
    "\n",
    "Define full quarter employment as \"paid by same employer in both the quarter before and after\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if employed full quarter either 2nd quarter or 3rd quarter of 2015\n",
    "full_qtr = df_jobs.pivot_table(index=['key_id', 'employer'], columns='quarter', values=['wages', 'weeks'])\n",
    "\n",
    "# index for full empl in 2nd quarter\n",
    "idx2 = full_qtr[[('wages', 1), ('wages', 2), ('wages', 3)]].notnull().sum(1)==3\n",
    "\n",
    "# index for full empl in 3rd quarter\n",
    "idx3 = full_qtr[[('wages', 2), ('wages', 3), ('wages', 4)]].notnull().sum(1)==3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_qtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_qtr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average full quarter earnings\n",
    "\n",
    "full_qtr['avg_full_qtr_wages'] = \\\n",
    "((full_qtr[('wages', 2)]*idx2) + (full_qtr[('wages', 3)]*idx3))\\\n",
    "/ (pd.np.ones(len(idx2))*idx2 + pd.np.ones(len(idx3))*idx3)\n",
    "\n",
    "# subset to only those fully employed in one or both quarters\n",
    "full_qtr = full_qtr[full_qtr['avg_full_qtr_wages'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_qtr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_qtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns for easier reference\n",
    "full_qtr.columns = ['_'.join([str(c) for c in col]).strip() for col in full_qtr.columns.values]\n",
    "full_qtr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_qtr.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(full_qtr['avg_full_qtr_wages_'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting Completed Graphs\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "When you are satisfied with your visualization, you may want to save a a copy outside of your notebook. You can do this with `matplotlib`'s savefig function. You simply need to run:\n",
    "\n",
    "plt.savefig(\"fileName.fileExtension\")\n",
    "\n",
    "The file extension is actually surprisingly important. Image formats like png and jpeg are actually **not ideal**. These file formats store your graph as a giant grid of pixels, which is space-efficient, but can't be edited later. Saving your visualizations instead as a PDF is strongly advised. PDFs are a type of vector image, which means all the components of the graph will be maintained.\n",
    "\n",
    "With PDFs, you can later open the image in a program like Adobe Illustrator and make changes like the size or typeface of your text, move your legends, or adjust the colors of your visual encodings. All of this would be impossible with a png or jpeg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Let's save the employement patterns heatmap we created earlier\n",
    "## below just copied and pasted from above:\n",
    "\n",
    "# Create the matplotlib object so we can tweak graph properties later\n",
    "fig, ax = plt.subplots(figsize = (14,8))\n",
    "\n",
    "# create the list of labels we want on our y-axis\n",
    "ylabs = ['{:.2f}%'.format(x*100) for x in df_tmp_top['pct_cohort']]\n",
    "\n",
    "# make the heatmap\n",
    "sns.heatmap(df_tmp_top[cols], linewidths=0.01, linecolor='grey', yticklabels=ylabs, cbar=False, cmap=\"Blues\")\n",
    "\n",
    "# make y-labels horizontal and change tickmark font size\n",
    "plt.yticks(rotation=360, fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "\n",
    "# add axis labels\n",
    "ax.set_ylabel('Percent of cohort', fontsize=16)\n",
    "ax.set_xlabel('Quarter', fontsize=16)\n",
    "\n",
    "## Data Sourcing:\n",
    "ax.annotate('Source: Ohio Longitudinal Data Archive', \n",
    "            xy=(0.5,-0.15), xycoords=\"axes fraction\", fontsize=12)\n",
    "\n",
    "## add a title\n",
    "fig.suptitle('Top 10 most common employment patterns of cohort', fontsize=18)\n",
    "ax.set_title('Blue is \"employed\" and white is \"not employed\"', fontsize=12)\n",
    "\n",
    "fig.savefig('./output/cohort_empl_patterns.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Data Visualization Package\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "You can read more about different options for data visualization in Python in the [Additional Resources](#Additional-Resources) section at the bottom of this notebook. \n",
    "\n",
    "`matplotlib` is very expressive, meaning it has functionality that can easily account for fine-tuned graph creation and adjustment. However, this also means that `matplotlib` is somewhat more complex to code.\n",
    "\n",
    "`seaborn` is a higher-level visualization module, which means it is much less expressive and flexible than matplotlib, but far more concise and easier to code.\n",
    "\n",
    "It may seem like we need to choose between these two approaches, but this is not the case! Since `seaborn` is itself written in `matplotlib` (you will sometimes see `seaborn` be called a `matplotlib` 'wrapper'), we can use `seaborn` for making graphs quickly and then `matplotlib` for specific adjustments. When you see `plt` referenced in the code below, we are using `matplotlib`'s pyplot submodule.\n",
    "\n",
    "\n",
    "`seaborn` also improves on `matplotlib` in important ways, such as the ability to more easily visualize regression model results, creating small multiples, enabling better color palettes, and improve default aesthetics. From [`seaborn`'s documentation](https://seaborn.pydata.org/introduction.html):\n",
    "\n",
    "> If matplotlib 'tries to make easy things easy and hard things possible', seaborn tries to make a well-defined set of hard things easy too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Important Note on Graph Titles\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The title of a visualization occupies the most valuable real estate on the page. If nothing else, you can be reasonably sure a viewer will at least read the title and glance at your visualization. This is why you want to put thought into making a clear and effective title that acts as a **narrative** for your chart. Many novice visualizers default to an **explanatory** title, something like: \"Average Wages Over Time (2006-2016)\". This title is correct - it just isn't very useful. This is particularly true since any good graph will have explained what the visualization is through the axes and legends. Instead, use the title to reinforce and explain the core point of the visualization. It should answer the question \"Why is this graph important?\" and focus the viewer onto the most critical take-away."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* [Data-Viz-Extras](../notebooks_additional/Data-Viz-extras.ipynb) notebook in the \"notebooks_additional\" folder\n",
    "\n",
    "* [A Thorough Comparison of Python's DataViz Modules](https://dsaber.com/2016/10/02/a-dramatic-tour-through-pythons-data-visualization-landscape-including-ggplot-and-altair)\n",
    "\n",
    "* [Seaborn Documentation](http://seaborn.pydata.org)\n",
    "\n",
    "* [Matplotlib Documentation](https://matplotlib.org)\n",
    "\n",
    "* [Advanced Functionality in Seaborn](blog.insightdatalabs.com/advanced-functionality-in-seaborn)\n",
    "\n",
    "* Other Python Visualization Libraries:\n",
    "    * [`Bokeh`](http://bokeh.pydata.org)\n",
    "    * [`Altair`](https://altair-viz.github.io)\n",
    "    * [`ggplot`](http://ggplot.yhathq.com.com)\n",
    "    * [`Plotly`](https://plot.ly)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-ada",
   "language": "python",
   "name": "py3-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "nav_menu": {
    "height": "272px",
    "width": "241px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "829px",
    "left": "0px",
    "right": "1548px",
    "top": "110px",
    "width": "301px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
