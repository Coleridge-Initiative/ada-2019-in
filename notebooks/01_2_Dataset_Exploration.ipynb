{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src=\"images/CI_horizontal.png\" width=\"600\">\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Website</a>\n",
    "    </span>\n",
    "</center>\n",
    "\n",
    "Ghani, Rayid, Frauke Kreuter, Julia Lane, Adrianne Bradford, Alex Engler, Nicolas Guetta Jeanrenaud, Graham Henke, Daniela Hochfellner, Clayton Hunter, Brian Kim, Avishek Kumar, and Jonathan Morgan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Exploration\n",
    "----------\n",
    "Basic dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\"><li><span><a href=\"#Dataset-Exploration\" data-toc-modified-id=\"Dataset-Exploration-1\">Dataset Exploration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\">Introduction</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning-Objectives\" data-toc-modified-id=\"Learning-Objectives-1.1.1\">Learning Objectives</a></span></li><li><span><a href=\"#Methods\" data-toc-modified-id=\"Methods-1.1.2\">Methods</a></span></li></ul></li><li><span><a href=\"#Python-Setup\" data-toc-modified-id=\"Python-Setup-1.2\">Python Setup</a></span></li><li><span><a href=\"#Load-the-Data\" data-toc-modified-id=\"Load-the-Data-1.3\">Load the Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Establish-a-Connection-to-the-Database\" data-toc-modified-id=\"Establish-a-Connection-to-the-Database-1.3.1\">Establish a Connection to the Database</a></span></li><li><span><a href=\"#Formulate-Data-Query\" data-toc-modified-id=\"Formulate-Data-Query-1.3.2\">Formulate Data Query</a></span></li><li><span><a href=\"#Pull-Data-from-the-Database\" data-toc-modified-id=\"Pull-Data-from-the-Database-1.3.3\">Pull Data from the Database</a></span></li></ul></li><li><span><a href=\"#Analysis:-Using-Python-and-SQL\" data-toc-modified-id=\"Analysis:-Using-Python-and-SQL-1.4\">Analysis: Using Python and SQL</a></span><ul class=\"toc-item\"><li><span><a href=\"#What-is-in-the-Database?\" data-toc-modified-id=\"What-is-in-the-Database?-1.4.1\">What is in the Database?</a></span></li></ul></li><li><span><a href=\"#Summary-Statistics\" data-toc-modified-id=\"Summary-Statistics-1.5\">Summary Statistics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Query-time-and-the-PostgreSQL-EXPLAIN-function\" data-toc-modified-id=\"Query-time-and-the-PostgreSQL-EXPLAIN-function-1.5.1\">Query time and the PostgreSQL EXPLAIN function</a></span></li></ul></li><li><span><a href=\"#Exploring-education-and-training-data\" data-toc-modified-id=\"Exploring-education-and-training-data-1.6\">Exploring education and training data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Ohio-data\" data-toc-modified-id=\"Ohio-data-1.6.1\">Ohio data</a></span></li><li><span><a href=\"#Employment-and-Education\" data-toc-modified-id=\"Employment-and-Education-1.6.2\">Employment and Education</a></span></li></ul></li><li><span><a href=\"#Creating-New-Measures\" data-toc-modified-id=\"Creating-New-Measures-1.7\">Creating New Measures</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In an ideal world, we will have all of the data we want with all of the desirable properties (no missing values, no errors, standard formats, and so on). \n",
    "However, that is hardly ever true - and we have to work with using our datasets to answer questions of interest as intelligently as possible. \n",
    "\n",
    "In this notebook, we will discover the datasets we have on the ADRF, and we will use our datasets to answer some questions of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "This notebook will give you the opportunity to spend some hands-on time with the data. \n",
    "\n",
    "You will have an opportunity to explore the different datasets in the ADRF, and this notebook will take you around the different ways you can analyze your data. This involves looking at basic metrics in the larger dataset, taking a random sample, creating derived variables, making sense of the missing values, and so on. \n",
    "\n",
    "This will be done using both SQL and `pandas` (a Python package). The `sqlalchemy` Python package provides a connetion to the database to pull data into Python. \n",
    "\n",
    "This notebook will provide an introduction and examples for: \n",
    "\n",
    "- How to create new tables from the larger tables in database (sometimes called the \"analytical frame\")\n",
    "- How to explore different variables of interest\n",
    "- How to explore aggregate metrics\n",
    "- How to handle missing values\n",
    "- How to join newly created tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We will be using the `sqlalchemy` Python package to access tables in our database server - PostgreSQL. \n",
    "\n",
    "To read the results of our queries, we will be using the `pandas` Python package, which has the ability to read tabular data from SQL queries into a pandas DataFrame object. Within `pandas`, we will use various commands:\n",
    "\n",
    "- Subsetting data\n",
    "- `groupby`\n",
    "- `merge`\n",
    "\n",
    "Within SQL, we will use various queries to:\n",
    "\n",
    "- select data subsets\n",
    "- Sum over groups\n",
    "- create new tables\n",
    "- Count distinct values of desired variables\n",
    "- Order data by chosen variables\n",
    "- Select a random sub-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In Python, we `import` packages. The `import` command allows us to use libraries created by others in our own work by \"importing\" them. You can think of importing a library as opening up a toolbox and pulling out a specific tool. Among the most famous Python packages:\n",
    "- `numpy` is short for \"numerical Python\". `numpy` is a lynchpin in Python's scientific computing stack. Its strengths include a powerful *N*-dimensional array object, and a large suite of functions for doing numerical computing. \n",
    "- `pandas` is a library in Python for data analysis that uses the DataFrame object (modeled after R DataFrames, for those familiar with that language) which is similiar to a spreedsheet but allows you to do your analysis programaticaly rather than the point-and-click of Excel. It is a lynchpin of the PyData stack and is built on top of `numpy`.  \n",
    "- `sqlalchemy` is a Python library for interfacing with a PostGreSQL database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas-related imports\n",
    "import pandas as pd\n",
    "\n",
    "# database interaction imports\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When in doubt, use shift + tab to read the documentation of a method.__\n",
    "\n",
    "__The `help()` function provides information on what you can do with a function.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for example\n",
    "help(sqlalchemy.create_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "We can execute SQL queries using Python to get the best of both worlds. For example, Python - and pandas in particular - make it much easier to calculate descriptive statistics of the data. Additionally, as we will see in the Data Visualization exercises, it is relatively easy to create data visualizations using Python. \n",
    "\n",
    "Pandas provides many ways to load data. It allows the user to read the data from a local csv or excel file, pull the data from a relational database, or read directly from a URL (when you have internet access). Since we are working with the PostgreSQL database `appliedda` in this course, we will demonstrate how to use pandas to read data from a relational database. For examples to read data from a CSV file, refert to the pandas documentation [Getting Data In/Out](pandas.pydata.org/pandas-docs/stable/10min.html#getting-data-in-out).\n",
    "\n",
    "The function to run a SQL query and pull the data into a pandas dataframe (more to come) is `pd.read_sql()`. Just like doing a SQL query from pgAdmin, this function will ask for some information about the database, and what query you would like to run. Let's walk through the example below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Establish a Connection to the Database\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "The first parameter is the connection to the database. To create a connection we will use the SQLAlchemy package and tell it which database we want to connect to, just like in pgAdmin. Additional details on creating a connection to the database are provided in the [Databases](02_1_Databases.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Database Connection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to create a connection to the database, \n",
    "# we need to pass the name of the database and host of the database\n",
    "\n",
    "host = 'stuffed.adrf.info'\n",
    "DB = 'appliedda'\n",
    "\n",
    "connection_string = \"postgresql://{}/{}\".format(host, DB)\n",
    "conn = sqlalchemy.create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note we can parameterize Python `string` objects - using the built-in `.format()` function. We will use various formulations in the program notebooks (eg when building queries), some examples are:\n",
    "1. Empty brackets (shown above) which simply inserts the variable in the string; when there is more than one set of brackets Python will insert variables in the order they are listed\n",
    "2. Brackets with formatting can be used to make print statements more readable (eg `'text with formatted number with comma and 1-digit decimal {:,.1f}'.format(number_value)` will print `123,456.7` instead of `123456.7123401`)\n",
    "3. Named brackets to use the same variables multiple times in a text block (we use this in more compicated queries eg when creating \"labels\" and \"features\" for Machine Learning models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulate Data Query\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__create a query as a `string` object in Python__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT *\n",
    "FROM in_data_2019.wages_by_employer\n",
    "WHERE year = 2014 AND quarter = 2\n",
    "LIMIT 20\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- the three quotation marks surrounding the query body is called multi-line string. It is quite handy for writing SQL queries because the new line character will be considered part of the string, instead of breaking the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have defined a variable `query`, we can call it in the code\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that the `LIMIT` provides one simple way to get a \"sample\" of data. However, using `LIMIT` does **not provide a _random_** sample; it is just based on what is fastest for the database to return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull Data from the Database\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "Now that we have the two parameters (database connection and query), we can pass them to the `pd.read_sql()` function, and obtain the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we pass the query and the connection to the pd.read_sql() function and assign the variable `wage` \n",
    "# to the dataframe returned by the function\n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis: Using Python and SQL\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "__What are different possible measures of employment for graduates?__\n",
    "\n",
    "To explore possible metrics, we will need to combine education and employment (in our case, UI wage records) data. We will start slow and explore these datasets individually, then work up to some initial metrics of employment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is in the Database?\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "As introduced in the [Databases](./02_1_Databases.ipynb) notebook, there are a few different ways to connect and explore the data in the database. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ Schemas, Tables, and Columns in database__\n",
    "\n",
    "Let's pull the list of schema names in the database, the list of tables in these schemas and the list of columns in these tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# See all available schemas:\n",
    "query = '''\n",
    "SELECT schema_name \n",
    "FROM information_schema.schemata;\n",
    "'''\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As a reminder, in this class you have access to the following schemas: 'public', 'data_ohio_olda_2018', 'il_des_kcmo', 'kcmo_lehd', 'ada_edwork' and your team schema ('ada_edwork_#', where the # is your team number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemas = \"\"\"\n",
    "'public', 'data_ohio_olda_2018', 'il_des_kcmo', 'kcmo_lehd', 'ada_edwork', 'in_data_2019'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm our schemas exist with \n",
    "# an updated version of the previous query\n",
    "query = '''\n",
    "SELECT schema_name \n",
    "FROM information_schema.schemata\n",
    "WHERE schema_name IN ({})\n",
    "'''.format(schemas)\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT schemaname, tablename\n",
    "FROM pg_tables\n",
    "WHERE schemaname IN ({})\n",
    "'''.format(schemas)\n",
    "\n",
    "tables = pd.read_sql(query, conn)\n",
    "# print tables not in the public schema\n",
    "print(tables.query(\"schemaname != 'public'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the tables in the Indiana schema:\n",
    "sorted(tables[tables[\"schemaname\"] == 'in_data_2019']['tablename'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note the two ways shown above to subset a `Pandas.DataFrame`:\n",
    "1. Use the built-in `.query()` function\n",
    "2. Create an array of `True` and `False` values (done in this line: `tables[\"schemaname\"] == 'in_data_2019'`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We can look at column names within tables\n",
    "# here we'll set the schema and table with variables\n",
    "\n",
    "schema = 'in_data_2019'\n",
    "tbl = 'wages_by_employer'\n",
    "\n",
    "query = '''\n",
    "SELECT * \n",
    "FROM information_schema.columns \n",
    "WHERE table_schema = '{}' AND table_name = '{}'\n",
    "'''.format(schema, tbl)\n",
    "\n",
    "# read and print results\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Indiana employment data__:\n",
    "- `in_data_2019.wages_by_employer`: individual job level data (row for every person paid by any company in the covered sector for each quarter)\n",
    "- `in_data_2019.wagesums`: individual employment summary for each quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT *\n",
    "FROM in_data_2019.wages_by_employer\n",
    "limit 100;\n",
    "'''\n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT *\n",
    "FROM in_data_2019.wagesums\n",
    "limit 100;\n",
    "'''\n",
    "df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "In this section, let's start looking at aggregate statistics on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore a few specific questions to better understand our data:\n",
    "- How are the data distributed across credential programs?\n",
    "- How many graduates from each of the programs get a job within one year?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: __ Large tables__ can take a long time to process on shared databases. The IL DES UI wage data has over 6M records per quarter, so we will demonstrate using SQL and Python with consideration for how much data we are reading back into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additionally we'll use the Python time package\n",
    "# to see how long different queries takes to return\n",
    "\n",
    "import time # import time package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example count of records for 2007 Q2 \n",
    "\n",
    "start_time = time.time() # get current time\n",
    "\n",
    "qry = \"\"\"\n",
    "SELECT count(*) \n",
    "FROM il_des_kcmo.il_wage         \n",
    "WHERE year = 2007 AND quarter = 2\n",
    "\"\"\"\n",
    "# print results\n",
    "print(pd.read_sql(qry, conn)) \n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example count of records for 2007 Q2 \n",
    "\n",
    "start_time = time.time() # get current time\n",
    "\n",
    "qry = \"\"\"\n",
    "SELECT count(*) \n",
    "FROM data_ohio_olda_2018.oh_ui_wage_by_employer\n",
    "WHERE year = 2007 AND quarter = 2\n",
    "\"\"\"\n",
    "# print results\n",
    "print(pd.read_sql(qry, conn)) \n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example count of records for 2007 Q2 \n",
    "\n",
    "start_time = time.time() # get current time\n",
    "\n",
    "qry = \"\"\"\n",
    "SELECT count(*) \n",
    "FROM in_data_2019.wages_by_employer\n",
    "WHERE year = 2007 AND quarter = 2\n",
    "\"\"\"\n",
    "# print results\n",
    "print(pd.read_sql(qry, conn)) \n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time() # get current time\n",
    "\n",
    "qry = \"\"\"\n",
    "SELECT count(*) \n",
    "FROM kcmo_lehd.mo_wage\n",
    "WHERE year = 2007 AND quarter = 2\n",
    "\"\"\"\n",
    "# print results\n",
    "print(pd.read_sql(qry, conn)) \n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A **question to consider**: This simple count is one measure of the total jobs in 2007 Q2. What may we want to consider when defining a \"job\" in addition to just being a row in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the easiest way to look at a small subset of records is what\n",
    "# is already demonstrated above: simply add a LIMIT clause\n",
    "# again we'll look at how long this query takes to return\n",
    "\n",
    "# get current time\n",
    "start_time = time.time()\n",
    "\n",
    "query = '''\n",
    "SELECT *\n",
    "FROM in_data_2019.wages_by_employer\n",
    "WHERE year = 2007 AND quarter = 2\n",
    "LIMIT 20;\n",
    "'''\n",
    "# get results\n",
    "df_earnings = pd.read_sql(query, conn)\n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_earnings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_earnings.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Reminder: you can refer to the documentation for more information on each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can get descriptive stats from the DataFrame:\n",
    "df_earnings.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as with the 'count` example, we will get basic stats\n",
    "# from the database using SQL; eg the wage distribution\n",
    "\n",
    "# first, just the 25th percentile value\n",
    "query = \"\"\"\n",
    "SELECT percentile_cont(0.25)\n",
    "    WITHIN GROUP (ORDER BY wages)\n",
    "FROM in_data_2019.wages_by_employer\n",
    "WHERE year = 2007 AND quarter = 2\n",
    "\"\"\"\n",
    "# display result\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: as of Jan 1, 2016, the minimum wage in IL was \\\\$8.25 per hour. Assuming a 35 hour work-week and 12 weeks in a quarter, someone working an entire quarter at minumum wage would earn \\\\$3,465 in the quarter (ignoring taxes). In 2007, IL minimum wage was \\\\$6.50 per hour, or \\\\$2,730 per quarter (with the same assumptions).\n",
    "> \n",
    "> Minimum wage in Ohio in 2016 was \\\\$8.10 per hour (\\\\$4.05 for tipped employees).\n",
    ">\n",
    "> As of Jan 1, 2015, minimum wage in Indiana was \\\\$7.25 per hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a list of percentile values at which to show the earnings value\n",
    "query = \"\"\"\n",
    "SELECT percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "WITHIN GROUP (ORDER BY wages)\n",
    "FROM in_data_2019.wages_by_employer\n",
    "WHERE year = 2007 AND quarter = 2\n",
    "\"\"\"\n",
    "# display result\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values above return in a single cell, \n",
    "# add \"unnest\" to get values in a single cell\n",
    "# also add a reference column and column names\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT unnest(\n",
    "        percentile_cont(array[0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "        WITHIN GROUP (ORDER BY wages)\n",
    "    ) AS earnings_value,\n",
    "    unnest(array[0.1, 0.25, 0.5, 0.75, 0.9]) AS percentile_value\n",
    "FROM in_data_2019.wages_by_employer\n",
    "WHERE year = 2007 AND quarter = 2\n",
    "\"\"\"\n",
    "# get the result\n",
    "df = pd.read_sql(query, conn)\n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))\n",
    "# view result\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to characterize a job is the employer industry. Note the minor differences in how the wage tables are presented from the different states: for both Illinois and Missouri, we have a different table for employers and for jobs (person<->employer combinations) and we need to get the NAICS code from the `<st>_qcew_employers` table to calculate industry earnings. In Ohio and Indiana, we do not have information about the employers beyond what is presented in the wage record data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many records from our Earnings data matches the Employers data\n",
    "# just for 2007 Q2 data\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "qry = \"\"\"\n",
    "SELECT count(*) \n",
    "FROM il_des_kcmo.il_wage AS earn\n",
    "JOIN il_des_kcmo.il_qcew_employers AS empl\n",
    "ON earn.ein = empl.ein \n",
    "    AND earn.seinunit = empl.seinunit\n",
    "    AND earn.empr_no = empl.empr_no\n",
    "WHERE earn.year = 2007 AND earn.quarter = 2 \n",
    "    AND empl.year = 2007 AND empl.quarter = 2\n",
    "\"\"\"\n",
    "res = pd.read_sql(qry, conn)\n",
    "print('query took {:.1f} seconds'.format(time.time()-start_time))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view number of wage records by NAICS code for 2007Q2\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "qry = \"\"\"\n",
    "SELECT empl.naics, count(*) \n",
    "FROM il_des_kcmo.il_wage AS earn\n",
    "JOIN il_des_kcmo.il_qcew_employers AS empl\n",
    "ON earn.ein = empl.ein \n",
    "    AND earn.seinunit = empl.seinunit\n",
    "    AND earn.empr_no = empl.empr_no\n",
    "WHERE earn.year = 2007 AND earn.quarter = 2 \n",
    "    AND empl.year = 2007 AND empl.quarter = 2\n",
    "GROUP BY naics\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "res = pd.read_sql(qry, conn)\n",
    "print('query took {:.1f} seconds'.format(time.time()-start_time))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the Ohio data\n",
    "start_time = time.time()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT naics_3_digit, count(*)\n",
    "FROM data_ohio_olda_2018.oh_ui_wage_by_employer\n",
    "WHERE year = 2007 AND quarter = 2\n",
    "GROUP BY naics_3_digit\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "# get the result\n",
    "df = pd.read_sql(query, conn)\n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))\n",
    "# view result\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note also that IL includes 6-digit NAICS while OH has 3-digit codes. The industry descriptions can be looked up in the `appliedda` database table `public.naics_<year>` (lookup tables are included for `year`s 2002, 2007, 2012, 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the Indiana data\n",
    "start_time = time.time()\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT naics_3_digit, count(*)\n",
    "FROM in_data_2019.wages_by_employer\n",
    "WHERE year = 2007 AND quarter = 2\n",
    "GROUP BY naics_3_digit\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "# get the result\n",
    "df = pd.read_sql(query, conn)\n",
    "# print analysis time\n",
    "print('Query returned in {:.1f} seconds'.format(time.time()-start_time))\n",
    "# view result\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query time and the PostgreSQL EXPLAIN function\n",
    "\n",
    "As you begin creating more advanced or complicated queries, it is useful to have a sense of how long different queries take. One way is to simple record and observe how long different queries take, as has been done in the notebook so far.\n",
    "\n",
    "You may get a sense of when a query is taking a long time. Slow queries could be the result of many people using the database at the same time, the database or table you are using are being `VACUUM`ed, or because of a poorly formed query or data structure (eg not making use of or having `indexes` on columns used to frequently subset or match the data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **The PostgreSQL \"Explain\" function** can help determine in what order the query is executed and whether it is making use of indexes. It is a little difficult to interpret, but here is an exmaple using the above query. The way to read this is start at the bottom of the RESULT and read up (steps outlined below the query output)\n",
    ">   \n",
    "    -- QUERY:\n",
    "    EXPLAIN \n",
    "    SELECT count(*) \n",
    "    FROM il_des_kcmo.il_wage AS earn\n",
    "    JOIN il_des_kcmo.il_qcew_employers AS empl\n",
    "    ON earn.ein = empl.ein AND earn.seinunit = empl.seinunit\n",
    "        AND earn.empr_no = empl.empr_no\n",
    "    WHERE earn.year = 2007 AND earn.quarter = 2 \n",
    "    AND empl.year = 2007 AND empl.quarter = 2\n",
    ">  \n",
    "    -- RESULT\n",
    "    Aggregate  (cost=2733713.11..2733713.12 rows=1 width=0)\n",
    "    ->  Merge Join  (cost=2659260.50..2733713.11 rows=1 width=0)\n",
    "         Merge Cond: ((earn.ein = empl.ein) AND (earn.seinunit = empl.seinunit) AND (earn.empr_no = empl.empr_no))\n",
    "         ->  Sort  (cost=1563641.97..1580871.12 rows=6891659 width=82)\n",
    "               Sort Key: earn.ein, earn.seinunit, earn.empr_no\n",
    "               ->  Append  (cost=0.00..451095.87 rows=6891659 width=82)\n",
    "                     ->  Seq Scan on il_wage earn  (cost=0.00..0.00 rows=1 width=96)\n",
    "                           Filter: ((year = 2007) AND (quarter = 2))\n",
    "                     ->  Seq Scan on il_wage_2007q2 earn_1  (cost=0.00..451095.87 rows=6891658 width=82)\n",
    "                           Filter: ((year = 2007) AND (quarter = 2))\n",
    "         ->  Materialize  (cost=1095618.53..1097832.93 rows=442881 width=80)\n",
    "               ->  Sort  (cost=1095618.53..1096725.73 rows=442881 width=80)\n",
    "                     Sort Key: empl.ein, empl.seinunit, empl.empr_no\n",
    "                     ->  Bitmap Heap Scan on il_qcew_employers empl  (cost=13816.09..1034403.41 rows=442881 width=80)\n",
    "                           Recheck Cond: ((year = 2007) AND (quarter = 2))\n",
    "                           ->  Bitmap Index Scan on il_des_kcmo_qcew_employers_year_quarter_index  (cost=0.00..13705.37 rows=442881 width=0)\n",
    "                                 Index Cond: ((year = 2007) AND (quarter = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this query runs in the following steps:\n",
    "1. scans the employer year and quarter indexes\n",
    "2. returns only rows where year and quarter meet our criteria\n",
    "3. sorts the result based on our 3 identifier columns\n",
    "4. performs same 3 steps on the earnings data (scan, filter, sort)\n",
    "5. merges the two tables based on the 3 identifier columns\n",
    "6. counts the rows (the final, \"Aggregate\" step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, try running explain on a slightly different variant of the previous query:\n",
    "\n",
    "    EXPLAIN SELECT count(*) \n",
    "    FROM il_des_kcmo.il_wage AS earn\n",
    "    JOIN il_des_kcmo.il_qcew_employers AS empl\n",
    "    ON earn.ein = empl.ein \n",
    "        AND earn.seinunit = empl.seinunit\n",
    "        AND earn.empr_no = empl.empr_no\n",
    "        AND earn.year = empl.year\n",
    "        AND earn.quarter = empl.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's peruse a sample of Indiana data\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "qry = \"\"\"\n",
    "SELECT *\n",
    "FROM in_data_2019.wages_by_employer\n",
    "WHERE year = 2007 AND quarter = 2 \n",
    "LIMIT 100\n",
    "\"\"\"\n",
    "res = pd.read_sql(qry, conn)\n",
    "\n",
    "# report how long it took to pull data\n",
    "print('query took {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the distribution of earnings in our sample?\n",
    "res['wages'].describe(percentiles=[0.1,0.25,0.5, 0.75, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# and earnings by industry?\n",
    "res.groupby('naics_3_digit')['wages'].describe(percentiles=[0.1,0.25,0.5, 0.75, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we'll query the database for earnings distribution by 2-digit NAICS in Q2 2007\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "query=\"\"\"\n",
    "SELECT naics2, \n",
    "    unnest(percentile_cont(array[0.1,0.25,0.5, 0.75, 0.9]) \n",
    "    within group (ORDER BY wages)) AS earnings,\n",
    "    unnest(array[0.1,0.25,0.5, 0.75, 0.9]) AS percentiles\n",
    "FROM (\n",
    "    SELECT *, left(naics_3_digit::text, 2) naics2\n",
    "    FROM in_data_2019.wages_by_employer\n",
    "    WHERE year = 2007 AND quarter = 2 \n",
    ") subquery\n",
    "GROUP BY naics2\n",
    "ORDER BY naics2, percentiles\n",
    "\"\"\"\n",
    "res = pd.read_sql(query, conn)\n",
    "print('query took {:.1f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can see from the count of non-null values in the `naics2` column if \n",
    "# there are some missing values in the data\n",
    "# if there are missing values we can view those rows with the following code\n",
    "\n",
    "res[res['naics2'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the null value (displayed by pandas as None) with an \"Unknown\" flag:\n",
    "res.naics2.fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DataFrames also have useful functions like pivot tables:\n",
    "res.pivot_table(values='earnings', columns='percentiles', index='naics2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The distribution of wages paid by different industry is one way to explore how the **jobs very across industry groups.**\n",
    "\n",
    "## Exploring education and training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get a reminder of what tables we have for Ohio\n",
    "\n",
    "query = '''\n",
    "SELECT table_name \n",
    "FROM information_schema.tables\n",
    "WHERE table_schema = 'in_data_2019'\n",
    "'''\n",
    "pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# che... is the Indiana Higher Ed data, see the columns\n",
    "\n",
    "query = '''\n",
    "SELECT column_name \n",
    "FROM information_schema.columns\n",
    "WHERE table_schema = 'in_data_2019'\n",
    "    AND table_name = 'che_completions'\n",
    "'''\n",
    "che_columns = pd.read_sql(query, conn)\n",
    "che_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the number of records per year\n",
    "# and confirm if there is a unique SSN per year\n",
    "\n",
    "start_time = time.time()\n",
    "query = \"\"\"\n",
    "select reporting_year, count(*) recs, count(distinct ssn) ind\n",
    "from in_data_2019.che_completions \n",
    "group by reporting_year\n",
    "order by reporting_year;\n",
    "\"\"\"\n",
    "print(pd.read_sql(query, conn))\n",
    "print('query completed in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> the \"ind\" count is slightly lower than the total \"recs\" count, indicating not each SSN is unique per year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OH HEI data is structured in \"wide\" format, where a given individual has one row with all of their information in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we can read all the columns into a dataframe\n",
    "query = '''\n",
    "SELECT column_name \n",
    "FROM information_schema.columns\n",
    "WHERE table_schema = 'data_ohio_olda_2018'\n",
    "    AND table_name = 'oh_hei'\n",
    "'''\n",
    "hei_columns = pd.read_sql(query, conn)\n",
    "hei_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_cols = [c for c in hei_columns['column_name'].values if c.startswith('deg') and c.endswith('_1')]\n",
    "len(deg_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This line of code may look complicated, so let's break it down step by step:\n",
    ">\n",
    "> 1. __`... for c in hei_columns['column_name'].values ...`__ - Loop through every element `c` in the list `hei_columns['column_name'].values`\n",
    "> 2. __`... if c.startswith('deg') and c.endswith('_1')`__ - Return only values that start with `deg` and end with `_1`\n",
    "> 3. __`c ...`__ - return value c in my new list\n",
    ">\n",
    "> _Additional Note: The formulation `[<action> for <item> in <iterable>]`is known as \"list comprehension\"._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore 2010 HEI data\n",
    "# cannot pull 1-year into memory, need subset of columns\n",
    "start_time = time.time()\n",
    "\n",
    "sql = \"\"\"\n",
    "SELECT *\n",
    "FROM (SELECT key_id, file_year AS year,\n",
    "        unnest(array[{col_str}]) AS col_name,\n",
    "        unnest(array[{col_val}]) AS col_value\n",
    "    FROM data_ohio_olda_2018.oh_hei \n",
    "    WHERE file_year = 2010\n",
    "    ) sub_query\n",
    "WHERE col_value IS NOT NULL AND col_value <> ''\n",
    "\"\"\".format(\n",
    "col_str=','.join([\"'\"+c+\"'\" for c in deg_cols]),\n",
    "col_val=','.join([c+'::text' for c in deg_cols])\n",
    ")\n",
    "df = pd.read_sql(sql, conn)\n",
    "# print(sql)\n",
    "\n",
    "print('query completed in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many people earned at least one degree/certificate in 2010?\n",
    "df['key_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['col_name'].str.contains('subject')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Subject codes can be looked up in the `data_ohio_olda_2018.oh_cip_to_soc_crosswalk` table; note the CIP codes are formatted `##.####`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many subjects were studied?\n",
    "df[df['col_name'].str.contains('subject')]['col_value'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Employment and Education\n",
    "\n",
    "Now we'll explore how many were **employed** ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "sql = \"\"\"\n",
    "CREATE TEMP TABLE oh_hei_2010 AS\n",
    "SELECT * \n",
    "FROM (SELECT key_id, file_year AS year,\n",
    "        unnest(array[{col_str}]) AS col_name,\n",
    "        unnest(array[{col_val}]) AS col_value\n",
    "    FROM data_ohio_olda_2018.oh_hei \n",
    "    WHERE file_year = 2010) sub_query\n",
    "WHERE col_value IS NOT NULL AND col_value <> ''\n",
    "\"\"\".format(\n",
    "col_str=','.join([\"'\"+c+\"'\" for c in deg_cols]),\n",
    "col_val=','.join([c+'::text' for c in deg_cols])\n",
    ")\n",
    "# print(sql)\n",
    "conn.execute(sql)\n",
    "\n",
    "print('query completed in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count records and distinct people in temp table to confirm it's the same as we expect\n",
    "pd.read_sql('SELECT count(*) recs, count(distinct key_id) people from oh_hei_2010', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many of the 2010 graduates were employed in 2011?\n",
    "start_time = time.time()\n",
    "\n",
    "sql = \"\"\"\n",
    "select count(*) job_qtrs, count(distinct key_id) employees\n",
    "from data_ohio_olda_2018.oh_ui_wage_by_quarter\n",
    "where year =2011\n",
    "and key_id IN (select distinct key_id from oh_hei_2010 )\n",
    "\"\"\"\n",
    "print(pd.read_sql(sql, conn))\n",
    "print('query completed in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating New Measures\n",
    "- Back to [Table of Contents](#Table-of-Contents)\n",
    "\n",
    "> **Questions to consider** (we will revisit similar questions of measurement frequently during the program)\n",
    "0. What problem are we working to solve? How can we measure it?\n",
    "1. How should we define that an individual \"received a job\"? For example, definitions could be \n",
    "  * Received greater than 0 pay at some point within 1 year\n",
    "  * Received greater than minimum wage (assuming XyZ) in 6 of 8 quarters after graduating\n",
    "2. How narrowly can you define the unit of analysis? Eg an individual who graduated in year Y...\n",
    "3. What additional information do we know about these individuals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preliminary Examples**\n",
    "\n",
    "As the notebooks progress we will dig into different aspects of the above questions, but for now we will show an example of defining graduates' employment outcomes: employed for at least 2 quarters the year after graduation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read 2011 jobs into a DataFrame\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "sql = \"\"\"\n",
    "select *\n",
    "from data_ohio_olda_2018.oh_ui_wage_by_quarter\n",
    "where year = 2011\n",
    "and key_id IN (select distinct key_id from oh_hei_2010 )\n",
    "\"\"\"\n",
    "\n",
    "df_jobs = pd.read_sql(sql, conn)\n",
    "\n",
    "print('query completed in {:.2f} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df_jobs['key_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:,.0f} of {:,.0f} 2010 graduates are present in OH 2011 wage data'.format(df_jobs['key_id'].nunique(), \n",
    "                                                                             df['key_id'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many were employed for 2+ quarters in 2011?\n",
    "\n",
    "sum(df_jobs.groupby('key_id')['id'].count() >= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# what are the distribution of earnings?\n",
    "df_jobs['sumwages'].describe(percentiles=[.1, .25, .5, .75, .9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many graduates made more than $3,000 in each of 2 or more quarters?\n",
    "sum(df_jobs[df_jobs['sumwages']>3000].groupby('key_id')['id'].count() > 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-ada",
   "language": "python",
   "name": "py3-ada"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "566px",
    "left": "0px",
    "right": "954px",
    "top": "110px",
    "width": "179px"
   },
   "toc_section_display": "none",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
